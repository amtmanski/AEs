{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchvision.utils import save_image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc31 = nn.Linear(256, 2)\n",
    "        self.fc32 = nn.Linear(256, 2)\n",
    "        self.fc4 = nn.Linear(2, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, input_shape)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h1 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h1), self.fc32(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        #adding stochastic part to variable z by eps\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn((std.shape[0], std.shape[1]))\n",
    "        z = mu + std*eps\n",
    "        return z\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        h2 = F.relu(self.fc4(z))\n",
    "        h2 = F.relu(self.fc5(h2))\n",
    "        return torch.sigmoid(self.fc6(h2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform)\n",
    "\n",
    "train_set, test_set = random_split(train_dataset, (48000, 12000))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = VAE(input_shape = 784)\n",
    "vae_optimizer = torch.optim.Adam(vae_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    #if reduction was left default the loss would stay the same \n",
    "    bce = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    #use KL-divergence to force 'z' mean=0 and std=1\n",
    "    kl_div = -0.5* torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return bce + kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, loss_fn, epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        for inputs, _ in train_loader:\n",
    "            #reshape input images\n",
    "            inputs = inputs.view(-1, 784)\n",
    "            #reser gradients\n",
    "            optimizer.zero_grad()\n",
    "            #compute reconstructions\n",
    "            outputs, mu, logvar = model(inputs)\n",
    "            #compute a training reconstruction loss\n",
    "            train_loss = loss_fn(outputs, inputs, mu, logvar)\n",
    "            #compute accumulated gradients\n",
    "            train_loss.backward()\n",
    "            #update parameters based on current gradients\n",
    "            optimizer.step()\n",
    "            #add the batch training loss to epoch loss\n",
    "            loss += train_loss.item()\n",
    "        #compute the epoch training loss\n",
    "        loss = loss/len(train_loader.dataset)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        model.eval()\n",
    "        for inputs, _ in val_loader:\n",
    "            #reshape input images\n",
    "            inputs = inputs.view(-1, 784)\n",
    "            #compute reconstruction\n",
    "            outputs, mu, logvar = model(inputs)\n",
    "            #compute validation loss\n",
    "            valid_loss += loss_fn(outputs, inputs, mu, logvar)\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        acc_list.append(valid_loss)\n",
    "        #print info\n",
    "        if epoch % 10 == 9 or epoch == 0:\n",
    "            print('Epoch {} of {}, training loss = {:.3f}, validation loss = {:.3f}'.format(epoch+1, epochs, loss, valid_loss))\n",
    "    return loss_list, acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100, training loss = 188.262, validation loss = 167.865\n",
      "Epoch 10 of 100, training loss = 144.032, validation loss = 145.279\n",
      "Epoch 20 of 100, training loss = 139.826, validation loss = 141.879\n",
      "Epoch 30 of 100, training loss = 137.444, validation loss = 140.699\n",
      "Epoch 40 of 100, training loss = 136.710, validation loss = 139.873\n",
      "Epoch 50 of 100, training loss = 135.115, validation loss = 139.022\n",
      "Epoch 60 of 100, training loss = 134.925, validation loss = 139.894\n"
     ]
    }
   ],
   "source": [
    "vae_loss_list, val_list = train(vae_model, train_loader, test_loader, vae_optimizer, vae_loss,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples(data_set, model):\n",
    "    fig, ax = plt.subplots(2,9, figsize=(20,5))\n",
    "    for i in range(9):\n",
    "        num = random.randint(0,len(data_set))\n",
    "        out, _, _ = model(data_set[num][0].view(-1, 784))\n",
    "        out = out/out.max().item()\n",
    "        out = out.view(28,28)\n",
    "        ax[0][i].imshow(data_set[num][0].reshape((28,28)), cmap='Greys_r')\n",
    "        ax[0][i].set_axis_off()\n",
    "        ax[1][i].imshow(out.detach(), cmap='Greys_r')\n",
    "        ax[1][i].set_axis_off()\n",
    "        if i%9 == 4:\n",
    "            ax[0][i].set_title('Original', size=20)\n",
    "            ax[1][i].set_title('Reconstructed', size=20)\n",
    "        plt.subplots_adjust(wspace = 0, hspace = 0.2)\n",
    "\n",
    "    plt.show()\n",
    "    #fig.savefig(file_name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_samples(test_set, vae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_images(model, n, digit_size=28):\n",
    "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
    "\n",
    "    norm = torch.distributions.Normal(0, 1)\n",
    "    grid_x = torch.tensor(np.linspace(-2, 2, n))\n",
    "    grid_y = torch.tensor(np.linspace(-2, 2, n))\n",
    "    image_width = digit_size*n\n",
    "    image_height = image_width\n",
    "    image = np.zeros((image_height, image_width))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z = torch.tensor(np.array([[xi, yi]]), dtype=torch.float32)\n",
    "            x_decoded = model.decoder(z)\n",
    "            digit = x_decoded.reshape((-1,digit_size, digit_size))\n",
    "            image[i * digit_size: (i + 1) * digit_size,\n",
    "                  j * digit_size: (j + 1) * digit_size] = digit.detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(image, cmap='Greys_r')\n",
    "    plt.axis('Off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_images(vae_model, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(vae_loss_list)), vae_loss_list, label='Training loss')\n",
    "plt.plot(range(len(val_list)), val_list, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
